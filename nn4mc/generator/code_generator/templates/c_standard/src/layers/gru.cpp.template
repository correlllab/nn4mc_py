<%BEGIN_DEFINITION_TEMPLATE>
/********************

    gru.cpp

    Code generated using nn4mc.

    This file implements a gru layer.

    Implementation from:
    Cho et. al (2014)
    https://arxiv.org/pdf/1406.1078.pdf
    https://github.com/keras-team/keras/blob/0a726d9844b4dce3efc5077718bdfb905e1750fa/keras/layers/recurrent.py#L1861

*********************/
#include "gru.h"

struct GRU build_layer_gru(
                          <%WEIGHT_DATATYPE_DELIMITER>* W,
                          <%WEIGHT_DATATYPE_DELIMITER>* U,
                          <%WEIGHT_DATATYPE_DELIMITER>* b,
                          <%ACTIVATION_DATATYPE_DELIMITER> recurrent_activation,
                          <%ACTIVATION_DATATYPE_DELIMITER> activation,
                          <%INDEX_DATATYPE_DELIMITER> input_shape_0,
                          <%INDEX_DATATYPE_DELIMITER> input_shape_1,
                          <%INDEX_DATATYPE_DELIMITER> output_units
){
	struct GRU layer;

	layer.weights = W;
    layer.big_u = U;
	layer.biases = b;

	layer.weight_shape[0] = input_shape_1;
	layer.weight_shape[1] = 3 * output_units;

    layer.big_u_shape[0] = output_units;
    layer.big_u_shape[1] = 3 * output_units;

    layer.biases_shape[0] = 2;
    layer.biases_shape[1] = 3 * output_units;

    layer.recurrent_activation = recurrent_activation;
    layer.activation = activation;

    layer.input_shape[0] = input_shape_0;
    layer.input_shape[1] = input_shape_1;

    layer.output_shape[0] = output_units;

    layer.h_tm1 = (<%LAYER_DATATYPE_DELIMITER>*)malloc(output_units * sizeof(<%LAYER_DATATYPE_DELIMITER>));
    for (<%INDEX_DATATYPE_DELIMITER> i = 0; i < output_units; i++){
        layer.h_tm1[i] = 0.0;
    }
	return layer;
}

<%LAYER_DATATYPE_DELIMITER> * fwd_gru(struct GRU L, <%LAYER_DATATYPE_DELIMITER> * input)
{
    const <%INDEX_DATATYPE_DELIMITER> M = L.output_shape[0];
    <%LAYER_DATATYPE_DELIMITER>* x_z = (<%LAYER_DATATYPE_DELIMITER>*)malloc(M * L.input_shape[0] * sizeof(<%LAYER_DATATYPE_DELIMITER>));
    <%LAYER_DATATYPE_DELIMITER>* x_r = (<%LAYER_DATATYPE_DELIMITER>*)malloc(M * L.input_shape[0] * sizeof(<%LAYER_DATATYPE_DELIMITER>));
    <%LAYER_DATATYPE_DELIMITER>* x_h = (<%LAYER_DATATYPE_DELIMITER>*)malloc(M * L.input_shape[0] * sizeof(<%LAYER_DATATYPE_DELIMITER>));
    <%LAYER_DATATYPE_DELIMITER>* h_t = (<%LAYER_DATATYPE_DELIMITER>*)malloc(M * L.input_shape[0] * sizeof(<%LAYER_DATATYPE_DELIMITER>));
    for (<%INDEX_DATATYPE_DELIMITER> i = 0; i < M*L.input_shape[0]; i++){
        x_z[i] = 0.0;
        x_r[i] = 0.0;
        x_h[i] = 0.0;
        h_t[i] = 0.0;
    }
    for (<%INDEX_DATATYPE_DELIMITER> i = 0; i < M; i++){
        for (<%INDEX_DATATYPE_DELIMITER> k = 0; k < L.input_shape[0]; k++){
            x_z[k * M + i] +=  L.biases[i];
            x_r[k * M + i] +=  L.biases[i + M];
            x_h[k * M + i] +=  L.biases[i + 2 * M];
            x_z[k * M + i] += L.biases[i + 3 * M];
            x_r[k * M + i] += L.biases[i + 4 * M];
            x_h[k * M + i] += L.biases[i + 5 * M];
            for (<%INDEX_DATATYPE_DELIMITER> j = 0; j < L.input_shape[1]; j++){
                x_z[k * M + i] += input[k * L.input_shape[1] + j] * *(L.weights + j * 3*M + i);
                x_r[k * M + i] += input[k * L.input_shape[1] + j] * *(L.weights + j * 3*M + i + M);
                x_h[k * M + i] += input[k * L.input_shape[1] + j] * *(L.weights + j * 3*M + i + 2*M);
            }
        }
        for (<%INDEX_DATATYPE_DELIMITER> j = 0; j < M; j++){
            x_z[i * M + j] += *(L.big_u + i * 3 * M + j) * L.h_tm1[j];
            x_r[i * M + j] += *(L.big_u + i * 3 * M + j + M) * L.h_tm1[j];
        }
    }
    x_z = activate(x_z, M * L.input_shape[0], L.recurrent_activation);
    x_r = activate(x_r, M * L.input_shape[0], L.recurrent_activation);
    for (<%INDEX_DATATYPE_DELIMITER> i = 0; i < M; i++){
        for (<%INDEX_DATATYPE_DELIMITER> j = 0; j < M; j++){
            x_h[i * M + j] += *(L.big_u + i * 3 * M + j + 2 * M) *
                                                    L.h_tm1[j] * x_r[j];
        }
    }
    x_h = activate(x_h, M * L.input_shape[0], L.activation);

    <%LAYER_DATATYPE_DELIMITER> one = 1.0;
    for (<%INDEX_DATATYPE_DELIMITER> i = 0; i < M; i++){
        for (<%INDEX_DATATYPE_DELIMITER> j = 0; j < L.input_shape[0]; j++){
                h_t[j * M + i] = (one - x_z[j * M + i]) *
                                        x_h[j * M + i] +
                                            x_z[j * M + i] *
                                            L.h_tm1[i];

            }

            L.h_tm1[i] = h_t[i];
    }
    free(x_z);
    free(x_r);
    free(x_h);
    free(input);
    return h_t;
}
<%END_DEFINITION_TEMPLATE>

<%BEGIN_INITIALIZE_TEMPLATE>
<%LAYER_NAME> = build_layer_gru(
                          &<%WEIGHT_NAME>[0],
                          &<%RECURRENT_WEIGHT_NAME>[0],
                          &<%BIAS_NAME>[0],
                          <%RECURRENT_ACTIVATION>,
                          <%ACTIVATION>,
                          <%INPUT_SHAPE_0>,
                          <%INPUT_SHAPE_1>,
                          <%OUTPUT_SHAPE>,
);
<%END_INITIALIZE_TEMPLATE>

<%BEGIN_CALL_TEMPLATE>
data =  fwd_gru(<%LAYER_NAME>, data);
<%END_CALL_TEMPLATE>