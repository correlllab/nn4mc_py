<%BEGIN_DEFINITION_TEMPLATE>
/********************
    gru.cpp

    Code generated using nn4mc.

    This file implements a gated recurrent unit layer.

*/

#include "gru.h"
#include "activations.h"
#include "functions.h"
#include <math.h>
#include <stdlib.h>

#define max(a, b) (((a)>(b) ? (a) : (b)))
#define min(a, b) (((a)<(b) ? (a) : (b)))

struct GRU buildGRU(<%WEIGHT_DATATYPE_DELIMITER> * W, <%WEIGHT_DATATYPE_DELIMITER> * Wrec, <%WEIGHT_DATATYPE_DELIMITER> * b, <%INDEX_DATATYPE_DELIMITER> input_sh0, <%INDEX_DATATYPE_DELIMITER> input_sh1, <%INDEX_DATATYPE_DELIMITER> output_sh, <%ACTIVATION_DATATYPE_DELIMITER> activation, <%ACTIVATION_DATATYPE_DELIMITER> recurrent_activation, <%LAYER_DATATYPE_DELIMITER> dropout,   <%LAYER_DATATYPE_DELIMITER>
recurrent_dropout, bool go_backwards)
{
	struct GRU layer;

	layer.weights = W;
	layer.biases = b;
    layer.wrec = Wrec;
    
    layer.input_shape[0] = input_sh0;
    layer.input_shape[1] = input_sh1;

    layer.activation = activation;
    layer.recurrent_activation = recurrent_activation;

    layer.output_size = output_sh;

    layer.dropout = dropout;
    layer.recurrent_dropout = recurrent_dropout;

    layer.go_backwards = go_backwards;

    layer.prev_h = (<%LAYER_DATATYPE_DELIMITER>*)malloc(layer.output_size *sizeof(<%LAYER_DATATYPE_DELIMITER>));
    for (int i = 0; i< layer.output_size; i++) layer.prev_h[i] = 0.0;

    return layer;
}

<%LAYER_DATATYPE_DELIMITER> * fwdGRU(struct GRU L, <%LAYER_DATATYPE_DELIMITER>* input)
{

    if (L.go_backwards){
        reverse(input, L.input_shape);
    }

    <%LAYER_DATATYPE_DELIMITER> * h = (<%LAYER_DATATYPE_DELIMITER>*)malloc(L.output_size*sizeof(<%LAYER_DATATYPE_DELIMITER>));

     <%LAYER_DATATYPE_DELIMITER> * z = (<%LAYER_DATATYPE_DELIMITER>*)malloc(L.output_size * sizeof(<%LAYER_DATATYPE_DELIMITER>));
     <%LAYER_DATATYPE_DELIMITER> * r = (<%LAYER_DATATYPE_DELIMITER>*)malloc(L.output_size * sizeof(<%LAYER_DATATYPE_DELIMITER>));
     <%LAYER_DATATYPE_DELIMITER> * hh = (<%LAYER_DATATYPE_DELIMITER>*)malloc(L.output_size * sizeof(<%LAYER_DATATYPE_DELIMITER>));

    for (int i=0; i< L.output_size; i++){
        <%INDEX_DATATYPE_DELIMITER> ind_z = i;
        <%INDEX_DATATYPE_DELIMITER> ind_r = 1*L.output_size + i;
        <%INDEX_DATATYPE_DELIMITER> ind_h = 2*L.output_size + i;

        z[i] = L.biases[ind_z];
        r[i] = L.biases[ind_r];
        hh[i] = L.biases[ind_h];

       for (<%INDEX_DATATYPE_DELIMITER> j=0; j< L.input_shape[0]*L.input_shape[1]; j++){
            z[i]+= *(L.weights + ind_z*L.output_size + j)*input[j];
            r[i]+= *(L.weights + ind_r*L.output_size + j)*input[j];
            hh[i]+= *(L.weights + ind_h*L.output_size + j)*input[j];
       }

       for (<%INDEX_DATATYPE_DELIMITER> j=0; j<L.output_size; j++){
            z[i]+= *(L.wrec + ind_z*L.output_size + j) * L.prev_h[j];
            r[i]+= *(L.wrec + ind_r*L.output_size + j) * L.prev_h[j];
       }

       for (<%INDEX_DATATYPE_DELIMITER> j=0;j<L.output_size;j++){
            z[i] = activate(z[i], L.output_size, L.recurrent_activation);
            r[i] = activate(r[i], L.output_size, L.recurrent_activation);
       }

       for (<%INDEX_DATATYPE_DELIMITER> j=0; j<L.output_size; j++){
            hh[i]+= *(L.wrec + ind_h*L.output_size + j) *(r[j]*L.prev_h[j]);
       }

      for (<%INDEX_DATATYPE_DELIMITER> j=0; j<L.output_size; j++){
            hh[i]= activate(hh[i], L.output_size, L.activation);
            h[i] = z[i]*L.prev_h[i] + (1 - z[i]) * hh[i];
            L.prev_h[i] = h[i];
       }        
      
    } 

    free(z);
    free(input);
    free(r);
    free(hh);
    
    return h;
}

<%END_DEFINITION_TEMPLATE>

<%BEGIN_INITIALIZE_TEMPLATE>
        <%LAYER_NAME> = buildGRU(&<%WEIGHT_NAME>[0], &<%RECURRENT_WEIGHT_NAME>[0], <%BIAS_NAME>, <%INPUT_SHAPE_0>, <%INPUT_SHAPE_1>, <%OUTPUT_SHAPE>, <%ACTIVATION>, <%RECURRENT_ACTIVATION>, <%DROPOUT>, <%RECURRENT_DROPOUT>, <%GO_BACKWARDS>);
<%END_INITIALIZE_TEMPLATE>

<%BEGIN_CALL_TEMPLATE>
        data = fwdGRU(<%LAYER_NAME>, data);
<%END_CALL_TEMPLATE>
